{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7639866,"sourceType":"datasetVersion","datasetId":841565}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport torch\n\nfrom transformers import LayoutLMTokenizerFast, LayoutLMForTokenClassification, Trainer, TrainingArguments\nfrom datasets import Dataset","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-27T22:28:43.771176Z","iopub.execute_input":"2024-07-27T22:28:43.772142Z","iopub.status.idle":"2024-07-27T22:28:44.007420Z","shell.execute_reply.started":"2024-07-27T22:28:43.772104Z","shell.execute_reply":"2024-07-27T22:28:44.006637Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Load the dataset\ndf = pd.read_csv('/kaggle/input/layoutlm/medquad.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-27T22:28:44.008999Z","iopub.execute_input":"2024-07-27T22:28:44.009391Z","iopub.status.idle":"2024-07-27T22:28:44.019638Z","shell.execute_reply.started":"2024-07-27T22:28:44.009362Z","shell.execute_reply":"2024-07-27T22:28:44.018689Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"                                 question  \\\n0                What is (are) Glaucoma ?   \n1                  What causes Glaucoma ?   \n2     What are the symptoms of Glaucoma ?   \n3  What are the treatments for Glaucoma ?   \n4                What is (are) Glaucoma ?   \n\n                                              answer           source  \\\n0  Glaucoma is a group of diseases that can damag...  NIHSeniorHealth   \n1  Nearly 2.7 million people have glaucoma, a lea...  NIHSeniorHealth   \n2  Symptoms of Glaucoma  Glaucoma can develop in ...  NIHSeniorHealth   \n3  Although open-angle glaucoma cannot be cured, ...  NIHSeniorHealth   \n4  Glaucoma is a group of diseases that can damag...  NIHSeniorHealth   \n\n  focus_area  \n0   Glaucoma  \n1   Glaucoma  \n2   Glaucoma  \n3   Glaucoma  \n4   Glaucoma  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>answer</th>\n      <th>source</th>\n      <th>focus_area</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>What is (are) Glaucoma ?</td>\n      <td>Glaucoma is a group of diseases that can damag...</td>\n      <td>NIHSeniorHealth</td>\n      <td>Glaucoma</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>What causes Glaucoma ?</td>\n      <td>Nearly 2.7 million people have glaucoma, a lea...</td>\n      <td>NIHSeniorHealth</td>\n      <td>Glaucoma</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>What are the symptoms of Glaucoma ?</td>\n      <td>Symptoms of Glaucoma  Glaucoma can develop in ...</td>\n      <td>NIHSeniorHealth</td>\n      <td>Glaucoma</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>What are the treatments for Glaucoma ?</td>\n      <td>Although open-angle glaucoma cannot be cured, ...</td>\n      <td>NIHSeniorHealth</td>\n      <td>Glaucoma</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>What is (are) Glaucoma ?</td>\n      <td>Glaucoma is a group of diseases that can damag...</td>\n      <td>NIHSeniorHealth</td>\n      <td>Glaucoma</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2024-07-27T22:28:44.020655Z","iopub.execute_input":"2024-07-27T22:28:44.020936Z","iopub.status.idle":"2024-07-27T22:28:44.039628Z","shell.execute_reply.started":"2024-07-27T22:28:44.020908Z","shell.execute_reply":"2024-07-27T22:28:44.038590Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 16412 entries, 0 to 16411\nData columns (total 4 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   question    16412 non-null  object\n 1   answer      16407 non-null  object\n 2   source      16412 non-null  object\n 3   focus_area  16398 non-null  object\ndtypes: object(4)\nmemory usage: 513.0+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":"df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2024-07-27T22:30:05.364710Z","iopub.execute_input":"2024-07-27T22:30:05.365323Z","iopub.status.idle":"2024-07-27T22:30:05.379765Z","shell.execute_reply.started":"2024-07-27T22:30:05.365293Z","shell.execute_reply":"2024-07-27T22:30:05.378626Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"question       0\nanswer         5\nsource         0\nfocus_area    14\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"# Drop rows with NaN values in the 'answer' column\ndf = df.dropna(subset=['answer'])\ndf.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2024-07-27T22:30:42.719767Z","iopub.execute_input":"2024-07-27T22:30:42.720172Z","iopub.status.idle":"2024-07-27T22:30:42.745976Z","shell.execute_reply.started":"2024-07-27T22:30:42.720134Z","shell.execute_reply":"2024-07-27T22:30:42.745110Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"question      0\nanswer        0\nsource        0\nfocus_area    0\ndtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"## Tokenization and Formatting\nUse a tokenizer from the Hugging Face Transformers library to tokenize the text.","metadata":{}},{"cell_type":"code","source":"# Initialize the tokenizer\ntokenizer = LayoutLMTokenizerFast.from_pretrained(\"microsoft/layoutlm-base-uncased\")\n\n# Create dummy labels for the example\ndf['labels'] = df['answer'].apply(lambda x: [0] * len(tokenizer.tokenize(x)))\n\n# Convert to Hugging Face Dataset\ndataset = Dataset.from_pandas(df)\n\ndef tokenize_and_format(examples):\n    tokenized_inputs = tokenizer(examples['answer'], padding='max_length', truncation=True, \n                                 max_length=512, return_offsets_mapping=True)\n    labels = []\n    for label in examples['labels']:\n        word_ids = tokenized_inputs.word_ids(batch_index=0)\n        # Adjust the length of the labels to match the word_ids\n        label_ids = [-100 if word_id is None else label[min(word_id, len(label) - 1)] for word_id in word_ids]\n        labels.append(label_ids)\n    tokenized_inputs[\"labels\"] = labels\n    tokenized_inputs.pop(\"offset_mapping\")  # Remove offset_mapping as it's not needed\n    return tokenized_inputs\n\n# Tokenize the dataset\ntokenized_dataset = dataset.map(tokenize_and_format, batched=True, remove_columns=dataset.column_names)\n\ntokenized_dataset.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-27T22:30:56.188738Z","iopub.execute_input":"2024-07-27T22:30:56.189067Z","iopub.status.idle":"2024-07-27T22:31:14.964728Z","shell.execute_reply.started":"2024-07-27T22:30:56.189042Z","shell.execute_reply":"2024-07-27T22:31:14.963741Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"0    [input_ids, token_type_ids, attention_mask, of...\n1    [input_ids, token_type_ids, attention_mask, of...\n2    [input_ids, token_type_ids, attention_mask, of...\n3    [input_ids, token_type_ids, attention_mask, of...\n4    [input_ids, token_type_ids, attention_mask, of...\ndtype: object\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Model Training\nPrepare the model and set up the training loop.","metadata":{}},{"cell_type":"code","source":"# Initialize the model\nmodel = LayoutLMForTokenClassification.from_pretrained(\"microsoft/layoutlm-base-uncased\", num_labels=2)\n\n# Set up training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    num_train_epochs=3,\n    weight_decay=0.01,\n)\n\n# Initialize Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n    eval_dataset=tokenized_dataset,\n)\n\n# Train the model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-07-27T22:35:30.684709Z","iopub.execute_input":"2024-07-27T22:35:30.685554Z","iopub.status.idle":"2024-07-27T23:51:14.668375Z","shell.execute_reply.started":"2024-07-27T22:35:30.685522Z","shell.execute_reply":"2024-07-27T23:51:14.667599Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (1191 > 512). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/16407 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49c6e2e3960843338ecf45315168e8ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/451M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"192ed94dcaab4360b78794f72c5e2024"}},"metadata":{}},{"name":"stderr","text":"Some weights of LayoutLMForTokenClassification were not initialized from the model checkpoint at microsoft/layoutlm-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.5 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.4"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240727_223905-ibdi14v3</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/amirakadry01-cairo-university/huggingface/runs/ibdi14v3' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/amirakadry01-cairo-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/amirakadry01-cairo-university/huggingface' target=\"_blank\">https://wandb.ai/amirakadry01-cairo-university/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/amirakadry01-cairo-university/huggingface/runs/ibdi14v3' target=\"_blank\">https://wandb.ai/amirakadry01-cairo-university/huggingface/runs/ibdi14v3</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6153' max='6153' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6153/6153 1:11:50, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.000000</td>\n      <td>0.000002</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.000000</td>\n      <td>0.000001</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.000000</td>\n      <td>0.000001</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=6153, training_loss=0.0002219991392513361, metrics={'train_runtime': 4498.2599, 'train_samples_per_second': 10.942, 'train_steps_per_second': 1.368, 'total_flos': 1.295058925587456e+16, 'train_loss': 0.0002219991392513361, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"## Evaluation\nEvaluate the model on the test set.","metadata":{}},{"cell_type":"code","source":"# Evaluate the model\nresults = trainer.evaluate()\nprint(results)","metadata":{"execution":{"iopub.status.busy":"2024-07-27T23:51:58.592942Z","iopub.execute_input":"2024-07-27T23:51:58.593575Z","iopub.status.idle":"2024-07-27T23:58:15.930008Z","shell.execute_reply.started":"2024-07-27T23:51:58.593541Z","shell.execute_reply":"2024-07-27T23:58:15.929065Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2051' max='2051' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2051/2051 06:17]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 1.080354650184745e-06, 'eval_runtime': 377.3254, 'eval_samples_per_second': 43.482, 'eval_steps_per_second': 5.436, 'epoch': 3.0}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Make predictions on new data.","metadata":{}},{"cell_type":"code","source":"# Predict on new data\nnew_text = [\"This is a new sentence for token classification.\"]\ntokenized_inputs = tokenizer(new_text, padding='max_length', truncation=True, max_length=512, return_tensors=\"pt\")\n\n# Ensure tensors are on the same device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\ntokenized_inputs = {k: v.to(device) for k, v in tokenized_inputs.items()}\n\nmodel.eval()\nwith torch.no_grad():\n    outputs = model(**tokenized_inputs)\n    predictions = outputs.logits.argmax(dim=-1)\n    print(predictions)","metadata":{"execution":{"iopub.status.busy":"2024-07-28T00:00:15.650998Z","iopub.execute_input":"2024-07-28T00:00:15.651989Z","iopub.status.idle":"2024-07-28T00:00:15.737581Z","shell.execute_reply.started":"2024-07-28T00:00:15.651953Z","shell.execute_reply":"2024-07-28T00:00:15.736468Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n","output_type":"stream"}]}]}