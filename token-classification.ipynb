{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7639866,"sourceType":"datasetVersion","datasetId":841565}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport torch\n\nfrom transformers import LayoutLMTokenizerFast, LayoutLMForTokenClassification, Trainer, TrainingArguments\nfrom datasets import Dataset","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-27T22:28:43.771176Z","iopub.execute_input":"2024-07-27T22:28:43.772142Z","iopub.status.idle":"2024-07-27T22:28:44.007420Z","shell.execute_reply.started":"2024-07-27T22:28:43.772104Z","shell.execute_reply":"2024-07-27T22:28:44.006637Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Load the dataset\ndf = pd.read_csv('/kaggle/input/layoutlm/medquad.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-27T22:28:44.008999Z","iopub.execute_input":"2024-07-27T22:28:44.009391Z","iopub.status.idle":"2024-07-27T22:28:44.019638Z","shell.execute_reply.started":"2024-07-27T22:28:44.009362Z","shell.execute_reply":"2024-07-27T22:28:44.018689Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"                                 question  \\\n0                What is (are) Glaucoma ?   \n1                  What causes Glaucoma ?   \n2     What are the symptoms of Glaucoma ?   \n3  What are the treatments for Glaucoma ?   \n4                What is (are) Glaucoma ?   \n\n                                              answer           source  \\\n0  Glaucoma is a group of diseases that can damag...  NIHSeniorHealth   \n1  Nearly 2.7 million people have glaucoma, a lea...  NIHSeniorHealth   \n2  Symptoms of Glaucoma  Glaucoma can develop in ...  NIHSeniorHealth   \n3  Although open-angle glaucoma cannot be cured, ...  NIHSeniorHealth   \n4  Glaucoma is a group of diseases that can damag...  NIHSeniorHealth   \n\n  focus_area  \n0   Glaucoma  \n1   Glaucoma  \n2   Glaucoma  \n3   Glaucoma  \n4   Glaucoma  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>answer</th>\n      <th>source</th>\n      <th>focus_area</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>What is (are) Glaucoma ?</td>\n      <td>Glaucoma is a group of diseases that can damag...</td>\n      <td>NIHSeniorHealth</td>\n      <td>Glaucoma</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>What causes Glaucoma ?</td>\n      <td>Nearly 2.7 million people have glaucoma, a lea...</td>\n      <td>NIHSeniorHealth</td>\n      <td>Glaucoma</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>What are the symptoms of Glaucoma ?</td>\n      <td>Symptoms of Glaucoma  Glaucoma can develop in ...</td>\n      <td>NIHSeniorHealth</td>\n      <td>Glaucoma</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>What are the treatments for Glaucoma ?</td>\n      <td>Although open-angle glaucoma cannot be cured, ...</td>\n      <td>NIHSeniorHealth</td>\n      <td>Glaucoma</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>What is (are) Glaucoma ?</td>\n      <td>Glaucoma is a group of diseases that can damag...</td>\n      <td>NIHSeniorHealth</td>\n      <td>Glaucoma</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2024-07-27T22:28:44.020655Z","iopub.execute_input":"2024-07-27T22:28:44.020936Z","iopub.status.idle":"2024-07-27T22:28:44.039628Z","shell.execute_reply.started":"2024-07-27T22:28:44.020908Z","shell.execute_reply":"2024-07-27T22:28:44.038590Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 16412 entries, 0 to 16411\nData columns (total 4 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   question    16412 non-null  object\n 1   answer      16407 non-null  object\n 2   source      16412 non-null  object\n 3   focus_area  16398 non-null  object\ndtypes: object(4)\nmemory usage: 513.0+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":"df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2024-07-27T22:30:05.364710Z","iopub.execute_input":"2024-07-27T22:30:05.365323Z","iopub.status.idle":"2024-07-27T22:30:05.379765Z","shell.execute_reply.started":"2024-07-27T22:30:05.365293Z","shell.execute_reply":"2024-07-27T22:30:05.378626Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"question       0\nanswer         5\nsource         0\nfocus_area    14\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"# Drop rows with NaN values in the 'answer' column\ndf = df.dropna(subset=['answer'])\ndf.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2024-07-27T22:30:42.719767Z","iopub.execute_input":"2024-07-27T22:30:42.720172Z","iopub.status.idle":"2024-07-27T22:30:42.745976Z","shell.execute_reply.started":"2024-07-27T22:30:42.720134Z","shell.execute_reply":"2024-07-27T22:30:42.745110Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"question      0\nanswer        0\nsource        0\nfocus_area    0\ndtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"## Tokenization and Formatting\nUse a tokenizer from the Hugging Face Transformers library to tokenize the text.","metadata":{}},{"cell_type":"code","source":"# Initialize the tokenizer\ntokenizer = LayoutLMTokenizerFast.from_pretrained(\"microsoft/layoutlm-base-uncased\")\n\n# Create dummy labels for the example\ndf['labels'] = df['answer'].apply(lambda x: [0] * len(tokenizer.tokenize(x)))\n\n# Convert to Hugging Face Dataset\ndataset = Dataset.from_pandas(df)\n\ndef tokenize_and_format(examples):\n    tokenized_inputs = tokenizer(examples['answer'], padding='max_length', truncation=True, \n                                 max_length=512, return_offsets_mapping=True)\n    labels = []\n    for label in examples['labels']:\n        word_ids = tokenized_inputs.word_ids(batch_index=0)\n        # Adjust the length of the labels to match the word_ids\n        label_ids = [-100 if word_id is None else label[min(word_id, len(label) - 1)] for word_id in word_ids]\n        labels.append(label_ids)\n    tokenized_inputs[\"labels\"] = labels\n    tokenized_inputs.pop(\"offset_mapping\")  # Remove offset_mapping as it's not needed\n    return tokenized_inputs\n\n# Tokenize the dataset\ntokenized_dataset = dataset.map(tokenize_and_format, batched=True, remove_columns=dataset.column_names)\n\ntokenized_dataset.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-27T22:30:56.188738Z","iopub.execute_input":"2024-07-27T22:30:56.189067Z","iopub.status.idle":"2024-07-27T22:31:14.964728Z","shell.execute_reply.started":"2024-07-27T22:30:56.189042Z","shell.execute_reply":"2024-07-27T22:31:14.963741Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"0    [input_ids, token_type_ids, attention_mask, of...\n1    [input_ids, token_type_ids, attention_mask, of...\n2    [input_ids, token_type_ids, attention_mask, of...\n3    [input_ids, token_type_ids, attention_mask, of...\n4    [input_ids, token_type_ids, attention_mask, of...\ndtype: object\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Model Training\nPrepare the model and set up the training loop.","metadata":{}},{"cell_type":"code","source":"# Initialize the model\nmodel = LayoutLMForTokenClassification.from_pretrained(\"microsoft/layoutlm-base-uncased\", num_labels=2)\n\n# Set up training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    num_train_epochs=3,\n    weight_decay=0.01,\n)\n\n# Initialize Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n    eval_dataset=tokenized_dataset,\n)\n\n# Train the model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-07-27T22:35:30.684709Z","iopub.execute_input":"2024-07-27T22:35:30.685554Z","iopub.status.idle":"2024-07-27T23:51:14.668375Z","shell.execute_reply.started":"2024-07-27T22:35:30.685522Z","shell.execute_reply":"2024-07-27T23:51:14.667599Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (1191 > 512). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/16407 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49c6e2e3960843338ecf45315168e8ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/451M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"192ed94dcaab4360b78794f72c5e2024"}},"metadata":{}},{"name":"stderr","text":"Some weights of LayoutLMForTokenClassification were not initialized from the model checkpoint at microsoft/layoutlm-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.5 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.4"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240727_223905-ibdi14v3</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/amirakadry01-cairo-university/huggingface/runs/ibdi14v3' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/amirakadry01-cairo-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/amirakadry01-cairo-university/huggingface' target=\"_blank\">https://wandb.ai/amirakadry01-cairo-university/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/amirakadry01-cairo-university/huggingface/runs/ibdi14v3' target=\"_blank\">https://wandb.ai/amirakadry01-cairo-university/huggingface/runs/ibdi14v3</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6153' max='6153' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6153/6153 1:11:50, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.000000</td>\n      <td>0.000002</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.000000</td>\n      <td>0.000001</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.000000</td>\n      <td>0.000001</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=6153, training_loss=0.0002219991392513361, metrics={'train_runtime': 4498.2599, 'train_samples_per_second': 10.942, 'train_steps_per_second': 1.368, 'total_flos': 1.295058925587456e+16, 'train_loss': 0.0002219991392513361, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"## Evaluation\nEvaluate the model on the test set.","metadata":{}},{"cell_type":"code","source":"# Evaluate the model\nresults = trainer.evaluate()\nprint(results)","metadata":{"execution":{"iopub.status.busy":"2024-07-27T23:51:58.592942Z","iopub.execute_input":"2024-07-27T23:51:58.593575Z","iopub.status.idle":"2024-07-27T23:58:15.930008Z","shell.execute_reply.started":"2024-07-27T23:51:58.593541Z","shell.execute_reply":"2024-07-27T23:58:15.929065Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2051' max='2051' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2051/2051 06:17]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 1.080354650184745e-06, 'eval_runtime': 377.3254, 'eval_samples_per_second': 43.482, 'eval_steps_per_second': 5.436, 'epoch': 3.0}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Make predictions on new data.","metadata":{}},{"cell_type":"code","source":"# Predict on new data\nnew_text = [\"This is a new sentence for token classification.\"]\ntokenized_inputs = tokenizer(new_text, padding='max_length', truncation=True, max_length=512, return_tensors=\"pt\")\n\n# Ensure tensors are on the same device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\ntokenized_inputs = {k: v.to(device) for k, v in tokenized_inputs.items()}\n\nmodel.eval()\nwith torch.no_grad():\n    outputs = model(**tokenized_inputs)\n    predictions = outputs.logits.argmax(dim=-1)\n    print(predictions)","metadata":{"execution":{"iopub.status.busy":"2024-07-28T00:00:15.650998Z","iopub.execute_input":"2024-07-28T00:00:15.651989Z","iopub.status.idle":"2024-07-28T00:00:15.737581Z","shell.execute_reply.started":"2024-07-28T00:00:15.651953Z","shell.execute_reply":"2024-07-28T00:00:15.736468Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n","output_type":"stream"}]}]}